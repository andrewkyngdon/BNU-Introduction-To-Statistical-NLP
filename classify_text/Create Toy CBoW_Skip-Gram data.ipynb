{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create toy CBoW and Skip-Gram datasets from a sample text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Sample text passage from here: http://www.tnewfields.info/Articles/sum1.htm\n",
    "\n",
    "sample_text = \"There are basically two types of auctions: ascending-bid auctions and descending-bid auctions. Ascending-bid auctions start out with a low bid for an object. The price of the object is gradually raised until only one bidder remains. By contrast, descending-bid auctions start out with a high bid and the price is progressively lowered until a customer expresses a willingness to purchase the object. Both procedures have a number of variants. For example, in some types of auctions a professional auctioneer declares the suggested bids. In other types of auctions, however, the customers make their own bids. Another variant, used at places such as eBay or Yahoo Auction, is called a 'buyout option'. A high price for an item is declared. Anyone willing to pay that price is guaranteed a purchase. This variant seems to appeal consumers who dislike uncertainty: for a fixed price they are guaranteed an object. 'Buyout options' are most commonly used if the seller has a stock of several copies of the same item.\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Some elementary text pre-processing\n",
    "\n",
    "text = sample_text.lower() # All lowercase\n",
    "\n",
    "text = word_tokenize(text) # Tokenise\n",
    "\n",
    "text = list(filter(lambda x: x not in string.punctuation, text)) # Remove punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create vocabulary\n",
    "\n",
    "vocab = sorted(list(set(text)))\n",
    "\n",
    "word_to_idx = {word: idx for idx, word in enumerate(vocab)}\n",
    "\n",
    "essay = []\n",
    "for word in text:\n",
    "    val = word_to_idx[word]\n",
    "    essay.append(val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create CBoW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Continuous Bag of Words with a window size of 3 words.\n",
    "\n",
    "dat = []\n",
    "lab = []\n",
    "\n",
    "for i in range(3, len(essay) - 3):\n",
    "    context = [essay[i - 3], essay[i - 2], essay[i - 1], essay[i + 1], essay[i + 2], essay[i + 3]]\n",
    "    target = essay[i]\n",
    "    dat.append(context)\n",
    "    lab.append(target)\n",
    "\n",
    "dat_np = np.asarray(dat)\n",
    "lab_np = np.asarray(lab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save to HDF5 database\n",
    "\n",
    "hdf_trn_file = \"CBoW.hdf5\"\n",
    "hdf_list_trn_file = \"CBoW_hdf5.txt\"\n",
    "\n",
    "with h5py.File(hdf_trn_file, \"w\") as f:\n",
    "    f.create_dataset(\"data\", data=dat_np)\n",
    "    f.create_dataset(\"label\", data=lab_np)\n",
    "    f.close()\n",
    "\n",
    "with open(hdf_list_trn_file, \"w\") as f:\n",
    "    f.write(hdf_trn_file)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Skip-Gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Skip-Gram (reproduced from this GitHub gist by Mateusz Bednarski: https://gist.github.com/mbednarski/da08eb297304f7a66a3840e857e060a0 ).\n",
    "\n",
    "window_size = 3\n",
    "\n",
    "word_pairs = []\n",
    "for centre_word_pos in range(len(essay)):\n",
    "    for w in range(-window_size, window_size + 1):\n",
    "        context_word_pos = centre_word_pos + w\n",
    "        if context_word_pos < 0 or context_word_pos >= len(essay) or centre_word_pos == context_word_pos:\n",
    "            continue\n",
    "        context_word_idx = essay[context_word_pos]\n",
    "        word_pairs.append((essay[centre_word_pos], context_word_idx))\n",
    "\n",
    "word_pairs = np.array(word_pairs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save to a HDF5 database\n",
    "\n",
    "np.random.shuffle(word_pairs)\n",
    "\n",
    "hdf_trn_file = \"skpgrm.hdf5\"\n",
    "hdf_list_trn_file = \"skpgrm_hdf5.txt\"\n",
    "\n",
    "with h5py.File(hdf_trn_file, \"w\") as f:\n",
    "    f.create_dataset(\"data\", data=word_pairs[:,0])\n",
    "    f.create_dataset(\"label\", data=word_pairs[:,1])\n",
    "    f.close()\n",
    "\n",
    "with open(hdf_list_trn_file, \"w\") as f:\n",
    "    f.write(hdf_trn_file)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create skip-gram with negative sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The method below creates a great number of negative samples. Select as many as you see fit. Alternatively, you may wish to use the function \"skipgrams()\" in the keras.preprocessing.sequence Python library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unique, counts = np.unique(np.asarray(essay), return_counts = True) # Get unigram frequencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "uni_probs = (counts**(3/4))/np.sum(counts**(3/4)) # Calculate sampling probabilities using Mikolov's equation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "neg_samp = np.random.choice(unique, len(word_pairs), p = uni_probs) # Obtain negative samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "neg_dat = np.stack((word_pairs[:,0],neg_samp),axis=1) # Create negative sampling dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out any positive samples created.\n",
    "ns = []\n",
    "for n in range(len(neg_dat)):\n",
    "    if word_pairs[n,0] ==  neg_dat[n,0] and word_pairs[n,1] ==  neg_dat[n,1]:\n",
    "        continue\n",
    "    ns.append(neg_dat[n])\n",
    "ns = np.asarray(ns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pos_labs = np.ones(len(word_pairs)) # Create label of '1' for positive samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "neg_labs = np.zeros(len(ns)) # Create label of '0' for negative samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_dat = np.concatenate((word_pairs,ns),axis=0) # concatenate positive and negative samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_labs = np.append(pos_labs,neg_labs) # join positive and negative labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_dat = np.c_[all_dat,all_labs] # Add labels to dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.shuffle(all_dat) # Shuffle the data before saving to database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save to a HDF5 database\n",
    "\n",
    "hdf_trn_file = \"trn_skpgrm_neg_samp.hdf5\"\n",
    "hdf_list_trn_file = \"trn_skpgrm_neg_samp_hdf5_list.txt\"\n",
    "\n",
    "with h5py.File(hdf_trn_file, \"w\") as f:\n",
    "    f.create_dataset(\"data\", data=all_dat[:,[0,1]])\n",
    "    f.create_dataset(\"label\", data=all_dat[:,2])\n",
    "    f.close()\n",
    "\n",
    "with open(hdf_list_trn_file, \"w\") as f:\n",
    "    f.write(hdf_trn_file)\n",
    "    f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
